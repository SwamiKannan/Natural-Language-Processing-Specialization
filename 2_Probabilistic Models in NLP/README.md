# Course 2: Probabilistic Models in NLP
In Course 2 of the Natural Language Processing Specialization, you will:<br>

a) Create a simple auto-correct algorithm using minimum edit distance and dynamic programming,<br>
b) Apply the Viterbi Algorithm for part-of-speech (POS) tagging, which is vital for computational linguistics,<br>
c) Write a better auto-complete algorithm using an N-gram language model, and <br>
d) Write your own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model.

**Week 1: Auto-correct using Minimum Edit Distance**

- Create a simple auto-correct algorithm using minimum edit distance and dynamic programming

**Week 2: Part-of-Speech (POS) Tagging**

- Apply the Viterbi algorithm for POS tagging, which is important for computational linguistics

**Week 3: N-gram Language Models**

- Write a better auto-complete algorithm using an N-gram model (similar models are used for translation, determining the author of a text, and speech recognition)

**Week 4: Word2Vec and Stochastic Gradient Descent**

- Write your own Word2Vec model that uses a neural network to compute word embeddings using a continuous bag-of-words model
